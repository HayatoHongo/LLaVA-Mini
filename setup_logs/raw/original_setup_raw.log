# 2 files changed. README.md and pyproject.toml
# README.md: Original: https://github.com/ictnlp/LLaVA-Mini/blob/47da1137f7b503be685ab5faa9f64a88b102e786/README.md
# README.md: Suggested version: https://github.com/HayatoHongo/LLaVA-Mini/blob/d6073451359f8cad17e5e2ac610c73ba7c52071e/README.md
# pyproject.toml: Original: https://github.com/ictnlp/LLaVA-Mini/blob/4510bacb4223f12760d8cf2ffc611bdd43df920a/pyproject.toml
# pyproject.toml: Suggested version: https://github.com/HayatoHongo/LLaVA-Mini/blob/285ab8ae2ed6532b26a6809e8c9cb4f61bf8f716/pyproject.toml


Last login: Sat Aug 30 20:05:36 on ttys059
hongohayato@hongousatsuhitononotobukkukonpyuta ~ % ssh ubuntu@129.213.151.56
Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-60-generic x86_64)
 .============.
 ||   __      ||    _                    _         _
 ||   \_\     ||   | |    __ _ _ __ ___ | |__   __| | __ _
 ||    \_\    ||   | |   / _` | '_ ` _ \| '_ \ / _` |/ _` |
 ||   /_λ_\   ||   | |__| (_| | | | | | | |_) | (_| | (_| |
 ||  /_/ \_\  ||   |_____\__,_|_| |_| |_|_.__/ \__,_|\__,_|
  .============.                                  GPU CLOUD

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

ubuntu@129-213-151-56:~$ conda create -n llavamini-test python=3.10 -y
conda: command not found
ubuntu@129-213-151-56:~$ git clone https://github.com/ictnlp/LLaVA-Mini.git
fatal: destination path 'LLaVA-Mini' already exists and is not an empty directory.
ubuntu@129-213-151-56:~$ ls
LLaVA-Mini  llava-virginia  miniconda.sh  miniconda3
ubuntu@129-213-151-56:~$ rm -rf LLaVA-Mini/
ubuntu@129-213-151-56:~$ ls
llava-virginia  miniconda.sh  miniconda3
ubuntu@129-213-151-56:~$ rm -rf miniconda3
ubuntu@129-213-151-56:~$ rm -rf miniconda.sh
ubuntu@129-213-151-56:~$ ls
llava-virginia
ubuntu@129-213-151-56:~$ git clone https://github.com/ictnlp/LLaVA-Mini.git
Cloning into 'LLaVA-Mini'...
remote: Enumerating objects: 296, done.
remote: Counting objects: 100% (27/27), done.
remote: Compressing objects: 100% (20/20), done.
remote: Total 296 (delta 13), reused 17 (delta 7), pack-reused 269 (from 2)
Receiving objects: 100% (296/296), 55.87 MiB | 84.63 MiB/s, done.
Resolving deltas: 100% (89/89), done.
ubuntu@129-213-151-56:~$ cd LLaVA-Mini/
ubuntu@129-213-151-56:~/LLaVA-Mini$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh
--2025-08-30 11:20:04--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.191.158, 104.16.32.241, 2606:4700::6810:20f1, ...
Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.191.158|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 162129736 (155M) [application/octet-stream]
Saving to: ‘/home/ubuntu/miniconda.sh’

/home/ubuntu/miniconda.sh 100%[==================================>] 154.62M   288MB/s    in 0.5s

2025-08-30 11:20:05 (288 MB/s) - ‘/home/ubuntu/miniconda.sh’ saved [162129736/162129736]

ubuntu@129-213-151-56:~/LLaVA-Mini$ bash ~/miniconda.sh

Welcome to Miniconda3 py313_25.7.0-2

In order to continue the installation process, please review the license
agreement.
Please, press ENTER to continue
>>>
MINICONDA END USER LICENSE AGREEMENT

Copyright Notice: Miniconda(R) (C) 2015, Anaconda, Inc.
All rights reserved. Miniconda(R) is licensed, not sold.

Redistribution and use in source and binary forms, with or without modification, are permitted provid
ed that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this list of conditions and
 the following disclaimer;

2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions
and the following disclaimer in the documentation and/or other materials provided with the distributi
on;

3. The name Anaconda, Inc. or Miniconda(R) may not be used to endorse or promote products derived fro
m this software without specific prior written permission from Anaconda, Inc.; and

4. Miniconda(R) may not be used to access or allow third parties to access Anaconda package repositor
ies if such use would circumvent paid licensing requirements or is otherwise restricted by the Anacon
da Terms of Service.

DISCLAIMER: THIS SOFTWARE IS PROVIDED BY ANACONDA "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCL
UDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOS
E , AND NON-INFRINGEMENT ARE DISCLAIMED. IN NO EVENT SHALL ANACONDA BE LIABLE FOR ANY DIRECT, INDIREC
T, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREME
NT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGL
IGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF MINICONDA(R), EVEN IF ADVISED OF THE POSSIB
ILITY OF SUCH DAMAGE.


Do you accept the license terms? [yes|no]
>>> yes

Miniconda3 will now be installed into this location:
/home/ubuntu/miniconda3

  - Press ENTER to confirm the location
  - Press CTRL-C to abort the installation
  - Or specify a different location below

[/home/ubuntu/miniconda3] >>>
PREFIX=/home/ubuntu/miniconda3
Unpacking bootstrapper...
Unpacking payload...

Installing base environment...


Downloading and Extracting Packages:


## Package Plan ##

  environment location: /home/ubuntu/miniconda3

  added / updated specs:
    - defaults/linux-64::_libgcc_mutex==0.1=main[md5=c3473ff8bdb3d124ed5ff11ec380d6f9]
    - defaults/linux-64::_openmp_mutex==5.1=1_gnu[md5=71d281e9c2192cb3fa425655a8defb85]
    - defaults/linux-64::anaconda-anon-usage==0.7.2=py313hfc0e8ea_100[md5=c6ea2f8456510ea049a5f05fead4b9b1]
    - defaults/linux-64::anaconda-auth==0.8.6=py313h06a4308_0[md5=1b575dad8e7084c59cfd8f8c4a2bd421]
    - defaults/linux-64::anaconda-cli-base==0.5.2=py313h06a4308_0[md5=d629fb0a0282b3adcf8f53ab82b63ad4]
    - defaults/linux-64::annotated-types==0.6.0=py313h06a4308_0[md5=4dc276db59e14eaf187426b0040eb209]
    - defaults/linux-64::boltons==25.0.0=py313h06a4308_0[md5=afd30cc40ab0edf4e246b901c8eca725]
    - defaults/linux-64::brotlicffi==1.0.9.2=py313h6a678d5_1[md5=256e218354e6ffcd32269368b5b9590d]
    - defaults/linux-64::bzip2==1.0.8=h5eee18b_6[md5=f21a3ff51c1b271977f53ce956a69297]
    - defaults/linux-64::c-ares==1.34.5=hef5626c_0[md5=4a0059c416e26b3c5e8a8fdf11dfa811]
    - defaults/linux-64::ca-certificates==2025.7.15=h06a4308_0[md5=a65eaddc4f9529b9c908f544ca50e7e0]
    - defaults/linux-64::certifi==2025.8.3=py313h06a4308_0[md5=1d24ffefc1375c97ae5c5175235aa724]
    - defaults/linux-64::cffi==1.17.1=py313h1fdaa30_1[md5=c5d4f727e7bccd4bd8d599e049853383]
    - defaults/linux-64::click==8.2.1=py313h06a4308_0[md5=a4fe8d908fee44662a5667b40506cba5]
    - defaults/linux-64::colorama==0.4.6=py313h06a4308_0[md5=9c0c672e71bd0523bdaa9376ea056ebd]
    - defaults/linux-64::conda-anaconda-tos==0.2.2=py313h06a4308_1[md5=48b92f96da51225fe57f3fe687eec943]
    - defaults/linux-64::conda-content-trust==0.2.0=py313h06a4308_1[md5=aefc551e21e13ecc1de44ee589aea77d]
    - defaults/linux-64::conda-package-handling==2.4.0=py313h06a4308_0[md5=76b17c263bc01eaffebcb8ba7ccd6ca9]
    - defaults/linux-64::conda-package-streaming==0.12.0=py313h06a4308_0[md5=adef8a1cc0462068a51c68144f05be3c]
    - defaults/linux-64::conda==25.7.0=py313h06a4308_0[md5=2137e3bdfcba2a2bef4531b76d2f07ea]
    - defaults/linux-64::cpp-expected==1.1.0=hdb19cb5_0[md5=3a195bcf47b691adb4a635a8b7f396f7]
    - defaults/linux-64::cryptography==45.0.5=py313hea9ce0a_0[md5=e70a5914e157eb138c4620aab7bc17d9]
    - defaults/linux-64::dbus==1.16.2=h5bd4931_0[md5=4c4aef417b613e7111d90cf2348b231a]
    - defaults/linux-64::distro==1.9.0=py313h06a4308_0[md5=1fd7ab065e9958e7bbd4d3fbf175742b]
    - defaults/linux-64::expat==2.7.1=h6a678d5_0[md5=269942a9f3f943e2e5d8a2516a861f7c]
    - defaults/linux-64::fmt==9.1.0=hdb19cb5_1[md5=4f12930203ff2d84df5d287af9b29858]
    - defaults/linux-64::frozendict==2.4.2=py313h06a4308_0[md5=31f7036f9bdd3592e660ecfc631f054f]
    - defaults/linux-64::icu==73.1=h6a678d5_0[md5=6d09df641fc23f7d277a04dc7ea32dd4]
    - defaults/linux-64::idna==3.7=py313h06a4308_0[md5=f3e7d3ba37b45ef8782dd643d7e9733b]
    - defaults/linux-64::jaraco.classes==3.4.0=py313h06a4308_0[md5=c4ea3f7208a811b569bbeae8c42c5b2d]
    - defaults/linux-64::jaraco.context==6.0.0=py313h06a4308_0[md5=568af8d43ff7c6e7ecdc725bdc860053]
    - defaults/linux-64::jaraco.functools==4.1.0=py313h06a4308_0[md5=472d048415f35e29b8b39a6cfc947ccd]
    - defaults/linux-64::jsonpatch==1.33=py313h06a4308_1[md5=8cb4b1755a4a8fd6a57883863236ef2b]
    - defaults/linux-64::jsonpointer==3.0.0=py313h06a4308_0[md5=af07fefbc353b9e3292ad9e0d563d9be]
    - defaults/linux-64::keyring==25.6.0=py313h06a4308_0[md5=611301eaa58f7a2c9eb37376dbd84fea]
    - defaults/linux-64::ld_impl_linux-64==2.40=h12ee557_0[md5=ee672b5f635340734f58d618b7bca024]
    - defaults/linux-64::libarchive==3.7.7=hfab0078_0[md5=c208e4f83c30250d659102b271e8d4e6]
    - defaults/linux-64::libcurl==8.14.1=hc1efc7f_1[md5=e2d948b1b2c79337d29d294bb3698e16]
    - defaults/linux-64::libev==4.33=h7f8727e_1[md5=5065620db4393fb549f30114a33897d1]
    - defaults/linux-64::libffi==3.4.4=h6a678d5_1[md5=70646cc713f0c43926cfdcfe9b695fe0]
    - defaults/linux-64::libgcc-ng==11.2.0=h1234567_1[md5=a87728dabf3151fb9cfa990bd2eb0464]
    - defaults/linux-64::libgomp==11.2.0=h1234567_1[md5=b372c0eea9b60732fdae4b817a63c8cd]
    - defaults/linux-64::libmamba==2.0.5=haf1ee3a_1[md5=0f1450592dc7555e315f5d684ce481eb]
    - defaults/linux-64::libmambapy==2.0.5=py313hdb19cb5_1[md5=c56ed0c34af1a360984e20bf13200007]
    - defaults/linux-64::libmpdec==4.0.0=h5eee18b_0[md5=feb10f42b1a7b523acbf85461be41a3e]
    - defaults/linux-64::libnghttp2==1.57.0=h2d74bed_0[md5=674871621300f54e7ffcf93e6e341638]
    - defaults/linux-64::libsolv==0.7.30=he621ea3_1[md5=6918c29c4b65f21c71aac679b49c02e4]
    - defaults/linux-64::libssh2==1.11.1=h251f7ec_0[md5=dd68c24355431c0543339dda1404129d]
    - defaults/linux-64::libstdcxx-ng==11.2.0=h1234567_1[md5=57623d10a70e09e1d048c2b2b6f4e2dd]
    - defaults/linux-64::libuuid==1.41.5=h5eee18b_0[md5=4a6a2354414c9080327274aa514e5299]
    - defaults/linux-64::libxcb==1.17.0=h9b100fa_0[md5=fdf0d380fa3809a301e2dbc0d5183883]
    - defaults/linux-64::libxml2==2.13.8=hfdd30dd_0[md5=6da89b526f12f128af101c3f70d12c6d]
    - defaults/linux-64::lz4-c==1.9.4=h6a678d5_1[md5=2ee58861f2b92b868ce761abb831819d]
    - defaults/linux-64::markdown-it-py==2.2.0=py313h06a4308_1[md5=0c3ee40437213e528c99bff0bcc5a942]
    - defaults/linux-64::mdurl==0.1.0=py313h06a4308_0[md5=5bca585008b9e6ce6997871309117479]
    - defaults/linux-64::menuinst==2.3.1=py313h06a4308_0[md5=215cd4c223b3ab647d025f347d35c058]
    - defaults/linux-64::more-itertools==10.3.0=py313h06a4308_0[md5=ee08e17a18339269f11ba3109f4fb225]
    - defaults/linux-64::ncurses==6.5=h7934f7d_0[md5=0abfc090299da4bb031b84c64309757b]
    - defaults/linux-64::nlohmann_json==3.11.2=h6a678d5_0[md5=36890f7abd98066b607211b1773e6343]
    - defaults/linux-64::openssl==3.0.17=h5eee18b_0[md5=c032152f4080dd61875d5047641c8bf2]
    - defaults/linux-64::packaging==25.0=py313h06a4308_0[md5=7396e4349c90a320cb4aa1edf684886d]
    - defaults/linux-64::pcre2==10.42=hebb0a14_1[md5=727e15c3cfa02b032da4eb0c1123e977]
    - defaults/linux-64::pkce==1.0.3=py313h06a4308_0[md5=a1e79da30a6c62c6398e9bb990ee9ed6]
    - defaults/linux-64::platformdirs==4.3.7=py313h06a4308_0[md5=6cdbb54f78166c07399004f961726487]
    - defaults/linux-64::pluggy==1.5.0=py313h06a4308_0[md5=ea142d52281afe8e9a79e4a33c023e41]
    - defaults/linux-64::pthread-stubs==0.3=h0ce48e5_1[md5=973a642312d2a28927aaf5b477c67250]
    - defaults/linux-64::pycosat==0.6.6=py313h5eee18b_2[md5=60ea7416e08d78563e31d16c333bd57f]
    - defaults/linux-64::pydantic-core==2.33.2=py313hc6f7160_0[md5=5d5a3d0469b467688a13f505e77639ef]
    - defaults/linux-64::pydantic-settings==2.6.1=py313h06a4308_0[md5=37546fa6cc30a5a794b9dd505368eb84]
    - defaults/linux-64::pydantic==2.11.7=py313h06a4308_0[md5=7c8e28d9eb0dbf8b3bea0aa9f28f8c17]
    - defaults/linux-64::pygments==2.19.1=py313h06a4308_0[md5=8fcbba71964f3d4d936e846c3bc9fb13]
    - defaults/linux-64::pyjwt==2.10.1=py313h06a4308_0[md5=2c17b6fb0a6100caada2e7ee72308bec]
    - defaults/linux-64::pysocks==1.7.1=py313h06a4308_0[md5=a493580ce366b942a8e1c00ee00c4182]
    - defaults/linux-64::python-dotenv==1.1.0=py313h06a4308_0[md5=33e2c8b6a83ba5bdbda0639bfb37496c]
    - defaults/linux-64::python==3.13.5=h4612cfd_100_cp313[md5=1adf42b71c42a4a540eae2c0026f02c3]
    - defaults/linux-64::python_abi==3.13=0_cp313[md5=d4009c49dd2b54ffded7f1365b5f6505]
    - defaults/linux-64::readchar==4.0.5=py313h06a4308_0[md5=42a02a52ac3364e165087e5c753b40ce]
    - defaults/linux-64::readline==8.3=hc2a1206_0[md5=8578e006d4ef5cb98a6cda232b3490f6]
    - defaults/linux-64::reproc-cpp==14.2.4=h6a678d5_2[md5=b03aa4903158279f003e7032ab9f5601]
    - defaults/linux-64::reproc==14.2.4=h6a678d5_2[md5=3c6dbc6c60b3897222d79359343e90fa]
    - defaults/linux-64::requests==2.32.4=py313h06a4308_0[md5=c1790ae25dc021a7c6073ace845d8f55]
    - defaults/linux-64::rich==13.9.4=py313h06a4308_0[md5=29573c0e57aea86c5d68759975638187]
    - defaults/linux-64::ruamel.yaml.clib==0.2.12=py313h5eee18b_0[md5=d9da5a7327cd6c9ee11d7b7ac1b7d082]
    - defaults/linux-64::ruamel.yaml==0.18.10=py313h5eee18b_0[md5=70c6985c0871d511a94afb1e8537943f]
    - defaults/linux-64::secretstorage==3.3.1=py313h06a4308_1[md5=bd9d99500717e83799f51f36825e6c2a]
    - defaults/linux-64::semver==3.0.2=py313h06a4308_1[md5=42fd2505022b95daa0061e091c9ddcda]
    - defaults/linux-64::setuptools==78.1.1=py313h06a4308_0[md5=8f8e1c1e3af9d2d371aaa0ee8316ae7c]
    - defaults/linux-64::shellingham==1.5.0=py313h06a4308_0[md5=fec77e08b241c9bae71940db00e14b65]
    - defaults/linux-64::simdjson==3.10.1=hdb19cb5_0[md5=7b2a4b5be590071e1b4dce77b413d26a]
    - defaults/linux-64::spdlog==1.11.0=hdb19cb5_0[md5=023cfe6341f1d90dee2016e608538537]
    - defaults/linux-64::sqlite==3.50.2=hb25bd0a_1[md5=6ac08aa6b5f14911039aa04b2b2c3350]
    - defaults/linux-64::tk==8.6.15=h54e0aa7_0[md5=1fa91e0c4fc9c9435eda3f1a25a676fd]
    - defaults/linux-64::tomli==2.2.1=py313h06a4308_0[md5=4ae62c3391e11617a9f6d9556d8062c7]
    - defaults/linux-64::tqdm==4.67.1=py313h7040dfc_0[md5=2efb4b3310f058cb95d326989bddc761]
    - defaults/linux-64::truststore==0.10.1=py313h06a4308_0[md5=fee67c1946f31ba7b45962c20b7b1d8c]
    - defaults/linux-64::typer==0.9.0=py313h06a4308_0[md5=7c1a4cd7cd5bc54f1460e0e2a8c5c0a3]
    - defaults/linux-64::typing-extensions==4.12.2=py313h06a4308_0[md5=f011fee76cd912d47567c6cedb0074c9]
    - defaults/linux-64::typing-inspection==0.4.0=py313h06a4308_0[md5=b9349b70ca836c55debbaa5b544b0c3c]
    - defaults/linux-64::typing_extensions==4.12.2=py313h06a4308_0[md5=e1a419944c11dc4592cdce114d0a3063]
    - defaults/linux-64::urllib3==2.5.0=py313h06a4308_0[md5=8739060177d7ed31cd7292efc5bcc91e]
    - defaults/linux-64::wheel==0.45.1=py313h06a4308_0[md5=29057e876eedce0e37c2388c138a19f9]
    - defaults/linux-64::xorg-libx11==1.8.12=h9b100fa_1[md5=6298b27afae6f49f03765b2a03df2fcb]
    - defaults/linux-64::xorg-libxau==1.0.12=h9b100fa_0[md5=a8005a9f6eb903e113cd5363e8a11459]
    - defaults/linux-64::xorg-libxdmcp==1.1.5=h9b100fa_0[md5=c284a09ddfba81d9c4e740110f09ea06]
    - defaults/linux-64::xorg-xorgproto==2024.1=h5eee18b_1[md5=412a0d97a7a51d23326e57226189da92]
    - defaults/linux-64::xz==5.6.4=h5eee18b_1[md5=3581505fa450962d631bd82b8616350e]
    - defaults/linux-64::yaml-cpp==0.8.0=h6a678d5_1[md5=015d2d74ad3c8e53eec3358637433718]
    - defaults/linux-64::zlib==1.2.13=h5eee18b_1[md5=92e42d8310108b0a440fb2e60b2b2a25]
    - defaults/linux-64::zstandard==0.23.0=py313h2c38b39_1[md5=5b4bd26f9d674331122482d3c2ac0afe]
    - defaults/linux-64::zstd==1.5.6=hc292b87_0[md5=78ae7abd3020b41f827b35085845e1b8]
    - defaults/noarch::archspec==0.2.3=pyhd3eb1b0_0[md5=13d01ee2d343d8539bb47055a6c0b5b2]
    - defaults/noarch::charset-normalizer==3.3.2=pyhd3eb1b0_0[md5=c6fea3691e85cf7f568b0618ec29fc4f]
    - defaults/noarch::conda-anaconda-telemetry==0.3.0=pyhd3eb1b0_1[md5=2ac481091c9cc4eec50cf2f0cc42744d]
    - defaults/noarch::conda-libmamba-solver==25.4.0=pyhd3eb1b0_0[md5=77ff27567894c7ea3eb908489e5d3e5c]
    - defaults/noarch::jeepney==0.7.1=pyhd3eb1b0_0[md5=f115ef0af90b59f35ef56743955979a4]
    - defaults/noarch::pip==25.1=pyhc872135_2[md5=2778327d2a700153fefe0e69438b18e1]
    - defaults/noarch::pybind11-abi==5=hd3eb1b0_0[md5=7f0df6639fdf60ccd3045ee6faedd32f]
    - defaults/noarch::pycparser==2.21=pyhd3eb1b0_0[md5=135a72ff2a31150a3a3ff0b1edd41ca9]
    - defaults/noarch::tzdata==2025b=h04d1e81_0[md5=1d027393db3427ab22a02aa44a56f143]


The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu
  anaconda-anon-usa~ pkgs/main/linux-64::anaconda-anon-usage-0.7.2-py313hfc0e8ea_100
  anaconda-auth      pkgs/main/linux-64::anaconda-auth-0.8.6-py313h06a4308_0
  anaconda-cli-base  pkgs/main/linux-64::anaconda-cli-base-0.5.2-py313h06a4308_0
  annotated-types    pkgs/main/linux-64::annotated-types-0.6.0-py313h06a4308_0
  archspec           pkgs/main/noarch::archspec-0.2.3-pyhd3eb1b0_0
  boltons            pkgs/main/linux-64::boltons-25.0.0-py313h06a4308_0
  brotlicffi         pkgs/main/linux-64::brotlicffi-1.0.9.2-py313h6a678d5_1
  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6
  c-ares             pkgs/main/linux-64::c-ares-1.34.5-hef5626c_0
  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.7.15-h06a4308_0
  certifi            pkgs/main/linux-64::certifi-2025.8.3-py313h06a4308_0
  cffi               pkgs/main/linux-64::cffi-1.17.1-py313h1fdaa30_1
  charset-normalizer pkgs/main/noarch::charset-normalizer-3.3.2-pyhd3eb1b0_0
  click              pkgs/main/linux-64::click-8.2.1-py313h06a4308_0
  colorama           pkgs/main/linux-64::colorama-0.4.6-py313h06a4308_0
  conda              pkgs/main/linux-64::conda-25.7.0-py313h06a4308_0
  conda-anaconda-te~ pkgs/main/noarch::conda-anaconda-telemetry-0.3.0-pyhd3eb1b0_1
  conda-anaconda-tos pkgs/main/linux-64::conda-anaconda-tos-0.2.2-py313h06a4308_1
  conda-content-tru~ pkgs/main/linux-64::conda-content-trust-0.2.0-py313h06a4308_1
  conda-libmamba-so~ pkgs/main/noarch::conda-libmamba-solver-25.4.0-pyhd3eb1b0_0
  conda-package-han~ pkgs/main/linux-64::conda-package-handling-2.4.0-py313h06a4308_0
  conda-package-str~ pkgs/main/linux-64::conda-package-streaming-0.12.0-py313h06a4308_0
  cpp-expected       pkgs/main/linux-64::cpp-expected-1.1.0-hdb19cb5_0
  cryptography       pkgs/main/linux-64::cryptography-45.0.5-py313hea9ce0a_0
  dbus               pkgs/main/linux-64::dbus-1.16.2-h5bd4931_0
  distro             pkgs/main/linux-64::distro-1.9.0-py313h06a4308_0
  expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0
  fmt                pkgs/main/linux-64::fmt-9.1.0-hdb19cb5_1
  frozendict         pkgs/main/linux-64::frozendict-2.4.2-py313h06a4308_0
  icu                pkgs/main/linux-64::icu-73.1-h6a678d5_0
  idna               pkgs/main/linux-64::idna-3.7-py313h06a4308_0
  jaraco.classes     pkgs/main/linux-64::jaraco.classes-3.4.0-py313h06a4308_0
  jaraco.context     pkgs/main/linux-64::jaraco.context-6.0.0-py313h06a4308_0
  jaraco.functools   pkgs/main/linux-64::jaraco.functools-4.1.0-py313h06a4308_0
  jeepney            pkgs/main/noarch::jeepney-0.7.1-pyhd3eb1b0_0
  jsonpatch          pkgs/main/linux-64::jsonpatch-1.33-py313h06a4308_1
  jsonpointer        pkgs/main/linux-64::jsonpointer-3.0.0-py313h06a4308_0
  keyring            pkgs/main/linux-64::keyring-25.6.0-py313h06a4308_0
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0
  libarchive         pkgs/main/linux-64::libarchive-3.7.7-hfab0078_0
  libcurl            pkgs/main/linux-64::libcurl-8.14.1-hc1efc7f_1
  libev              pkgs/main/linux-64::libev-4.33-h7f8727e_1
  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1
  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1
  libmamba           pkgs/main/linux-64::libmamba-2.0.5-haf1ee3a_1
  libmambapy         pkgs/main/linux-64::libmambapy-2.0.5-py313hdb19cb5_1
  libmpdec           pkgs/main/linux-64::libmpdec-4.0.0-h5eee18b_0
  libnghttp2         pkgs/main/linux-64::libnghttp2-1.57.0-h2d74bed_0
  libsolv            pkgs/main/linux-64::libsolv-0.7.30-he621ea3_1
  libssh2            pkgs/main/linux-64::libssh2-1.11.1-h251f7ec_0
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1
  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0
  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0
  libxml2            pkgs/main/linux-64::libxml2-2.13.8-hfdd30dd_0
  lz4-c              pkgs/main/linux-64::lz4-c-1.9.4-h6a678d5_1
  markdown-it-py     pkgs/main/linux-64::markdown-it-py-2.2.0-py313h06a4308_1
  mdurl              pkgs/main/linux-64::mdurl-0.1.0-py313h06a4308_0
  menuinst           pkgs/main/linux-64::menuinst-2.3.1-py313h06a4308_0
  more-itertools     pkgs/main/linux-64::more-itertools-10.3.0-py313h06a4308_0
  ncurses            pkgs/main/linux-64::ncurses-6.5-h7934f7d_0
  nlohmann_json      pkgs/main/linux-64::nlohmann_json-3.11.2-h6a678d5_0
  openssl            pkgs/main/linux-64::openssl-3.0.17-h5eee18b_0
  packaging          pkgs/main/linux-64::packaging-25.0-py313h06a4308_0
  pcre2              pkgs/main/linux-64::pcre2-10.42-hebb0a14_1
  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2
  pkce               pkgs/main/linux-64::pkce-1.0.3-py313h06a4308_0
  platformdirs       pkgs/main/linux-64::platformdirs-4.3.7-py313h06a4308_0
  pluggy             pkgs/main/linux-64::pluggy-1.5.0-py313h06a4308_0
  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1
  pybind11-abi       pkgs/main/noarch::pybind11-abi-5-hd3eb1b0_0
  pycosat            pkgs/main/linux-64::pycosat-0.6.6-py313h5eee18b_2
  pycparser          pkgs/main/noarch::pycparser-2.21-pyhd3eb1b0_0
  pydantic           pkgs/main/linux-64::pydantic-2.11.7-py313h06a4308_0
  pydantic-core      pkgs/main/linux-64::pydantic-core-2.33.2-py313hc6f7160_0
  pydantic-settings  pkgs/main/linux-64::pydantic-settings-2.6.1-py313h06a4308_0
  pygments           pkgs/main/linux-64::pygments-2.19.1-py313h06a4308_0
  pyjwt              pkgs/main/linux-64::pyjwt-2.10.1-py313h06a4308_0
  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py313h06a4308_0
  python             pkgs/main/linux-64::python-3.13.5-h4612cfd_100_cp313
  python-dotenv      pkgs/main/linux-64::python-dotenv-1.1.0-py313h06a4308_0
  python_abi         pkgs/main/linux-64::python_abi-3.13-0_cp313
  readchar           pkgs/main/linux-64::readchar-4.0.5-py313h06a4308_0
  readline           pkgs/main/linux-64::readline-8.3-hc2a1206_0
  reproc             pkgs/main/linux-64::reproc-14.2.4-h6a678d5_2
  reproc-cpp         pkgs/main/linux-64::reproc-cpp-14.2.4-h6a678d5_2
  requests           pkgs/main/linux-64::requests-2.32.4-py313h06a4308_0
  rich               pkgs/main/linux-64::rich-13.9.4-py313h06a4308_0
  ruamel.yaml        pkgs/main/linux-64::ruamel.yaml-0.18.10-py313h5eee18b_0
  ruamel.yaml.clib   pkgs/main/linux-64::ruamel.yaml.clib-0.2.12-py313h5eee18b_0
  secretstorage      pkgs/main/linux-64::secretstorage-3.3.1-py313h06a4308_1
  semver             pkgs/main/linux-64::semver-3.0.2-py313h06a4308_1
  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py313h06a4308_0
  shellingham        pkgs/main/linux-64::shellingham-1.5.0-py313h06a4308_0
  simdjson           pkgs/main/linux-64::simdjson-3.10.1-hdb19cb5_0
  spdlog             pkgs/main/linux-64::spdlog-1.11.0-hdb19cb5_0
  sqlite             pkgs/main/linux-64::sqlite-3.50.2-hb25bd0a_1
  tk                 pkgs/main/linux-64::tk-8.6.15-h54e0aa7_0
  tomli              pkgs/main/linux-64::tomli-2.2.1-py313h06a4308_0
  tqdm               pkgs/main/linux-64::tqdm-4.67.1-py313h7040dfc_0
  truststore         pkgs/main/linux-64::truststore-0.10.1-py313h06a4308_0
  typer              pkgs/main/linux-64::typer-0.9.0-py313h06a4308_0
  typing-extensions  pkgs/main/linux-64::typing-extensions-4.12.2-py313h06a4308_0
  typing-inspection  pkgs/main/linux-64::typing-inspection-0.4.0-py313h06a4308_0
  typing_extensions  pkgs/main/linux-64::typing_extensions-4.12.2-py313h06a4308_0
  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0
  urllib3            pkgs/main/linux-64::urllib3-2.5.0-py313h06a4308_0
  wheel              pkgs/main/linux-64::wheel-0.45.1-py313h06a4308_0
  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1
  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0
  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0
  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1
  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1
  yaml-cpp           pkgs/main/linux-64::yaml-cpp-0.8.0-h6a678d5_1
  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1
  zstandard          pkgs/main/linux-64::zstandard-0.23.0-py313h2c38b39_1
  zstd               pkgs/main/linux-64::zstd-1.5.6-hc292b87_0



Downloading and Extracting Packages:

Preparing transaction: done
Executing transaction: done
installation finished.
Do you wish to update your shell profile to automatically initialize conda?
This will activate conda on startup and change the command prompt when activated.
If you'd prefer that conda's base environment not be activated on startup,
   run the following command when conda is activated:

conda config --set auto_activate_base false

You can undo this by running `conda init --reverse $SHELL`? [yes|no]
[no] >>>

You have chosen to not have conda modify your shell scripts at all.
To activate conda's base environment in your current shell session:

eval "$(/home/ubuntu/miniconda3/bin/conda shell.YOUR_SHELL_NAME hook)"

To install conda's shell functions for easier access, first activate, then:

conda init

Thank you for installing Miniconda3!
ubuntu@129-213-151-56:~/LLaVA-Mini$ source ~/.bashrc
ubuntu@129-213-151-56:~/LLaVA-Mini$ source ~/miniconda3/etc/profile.d/conda.sh
ubuntu@129-213-151-56:~/LLaVA-Mini$ conda --version
conda 25.7.0
ubuntu@129-213-151-56:~/LLaVA-Mini$ conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main
conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r
accepted Terms of Service for https://repo.anaconda.com/pkgs/main
accepted Terms of Service for https://repo.anaconda.com/pkgs/r
ubuntu@129-213-151-56:~/LLaVA-Mini$ conda create -n llavamini python=3.10 -y
2 channel Terms of Service accepted
Channels:
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: done

## Package Plan ##

  environment location: /home/ubuntu/miniconda3/envs/llavamini

  added / updated specs:
    - python=3.10


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    python-3.10.18             |       h1a3bd86_0        26.5 MB
    setuptools-78.1.1          |  py310h06a4308_0         1.7 MB
    wheel-0.45.1               |  py310h06a4308_0         115 KB
    ------------------------------------------------------------
                                           Total:        28.4 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu
  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6
  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.7.15-h06a4308_0
  expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0
  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1
  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1
  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0
  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0
  ncurses            pkgs/main/linux-64::ncurses-6.5-h7934f7d_0
  openssl            pkgs/main/linux-64::openssl-3.0.17-h5eee18b_0
  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2
  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1
  python             pkgs/main/linux-64::python-3.10.18-h1a3bd86_0
  readline           pkgs/main/linux-64::readline-8.3-hc2a1206_0
  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py310h06a4308_0
  sqlite             pkgs/main/linux-64::sqlite-3.50.2-hb25bd0a_1
  tk                 pkgs/main/linux-64::tk-8.6.15-h54e0aa7_0
  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0
  wheel              pkgs/main/linux-64::wheel-0.45.1-py310h06a4308_0
  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1
  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0
  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0
  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1
  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1
  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1



Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate llavamini
#
# To deactivate an active environment, use
#
#     $ conda deactivate

ubuntu@129-213-151-56:~/LLaVA-Mini$ conda activate llavamini
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ pip install -e .
Obtaining file:///home/ubuntu/LLaVA-Mini
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... done
Collecting torch==2.1.2 (from llava_mini==1.0.0)
  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)
Collecting torchvision==0.16.2 (from llava_mini==1.0.0)
  Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)
Collecting transformers==4.43.1 (from llava_mini==1.0.0)
  Using cached transformers-4.43.1-py3-none-any.whl.metadata (43 kB)
Collecting tokenizers==0.19.0 (from llava_mini==1.0.0)
  Using cached tokenizers-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting sentencepiece==0.1.99 (from llava_mini==1.0.0)
  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting shortuuid (from llava_mini==1.0.0)
  Using cached shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)
Collecting accelerate==0.29.0 (from llava_mini==1.0.0)
  Using cached accelerate-0.29.0-py3-none-any.whl.metadata (18 kB)
Collecting peft (from llava_mini==1.0.0)
  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)
Collecting bitsandbytes (from llava_mini==1.0.0)
  Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)
Collecting pydantic (from llava_mini==1.0.0)
  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)
Collecting markdown2[all] (from llava_mini==1.0.0)
  Using cached markdown2-2.5.4-py3-none-any.whl.metadata (2.1 kB)
Collecting numpy (from llava_mini==1.0.0)
  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)
Collecting scikit-learn==1.2.2 (from llava_mini==1.0.0)
  Using cached scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)
Collecting gradio==5.9.1 (from llava_mini==1.0.0)
  Using cached gradio-5.9.1-py3-none-any.whl.metadata (16 kB)
Collecting gradio_client==1.5.2 (from llava_mini==1.0.0)
  Using cached gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)
Collecting requests (from llava_mini==1.0.0)
  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting httpx==0.28.1 (from llava_mini==1.0.0)
  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)
Collecting uvicorn (from llava_mini==1.0.0)
  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)
Collecting fastapi (from llava_mini==1.0.0)
  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)
Collecting einops==0.6.1 (from llava_mini==1.0.0)
  Using cached einops-0.6.1-py3-none-any.whl.metadata (12 kB)
Collecting einops-exts==0.0.4 (from llava_mini==1.0.0)
  Using cached einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)
Collecting timm==0.6.13 (from llava_mini==1.0.0)
  Using cached timm-0.6.13-py3-none-any.whl.metadata (38 kB)
Collecting packaging>=20.0 (from accelerate==0.29.0->llava_mini==1.0.0)
  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting psutil (from accelerate==0.29.0->llava_mini==1.0.0)
  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting pyyaml (from accelerate==0.29.0->llava_mini==1.0.0)
  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting huggingface-hub (from accelerate==0.29.0->llava_mini==1.0.0)
  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)
Collecting safetensors>=0.3.1 (from accelerate==0.29.0->llava_mini==1.0.0)
  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting aiofiles<24.0,>=22.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)
Collecting anyio<5.0,>=3.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)
Collecting ffmpy (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached ffmpy-0.6.1-py3-none-any.whl.metadata (2.9 kB)
Collecting jinja2<4.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting markupsafe~=2.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)
Collecting orjson~=3.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached orjson-3.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)
Collecting pandas<3.0,>=1.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached pandas-2.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)
Collecting pillow<12.0,>=8.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)
Collecting pydub (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting python-multipart>=0.0.18 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)
Collecting ruff>=0.2.2 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached ruff-0.12.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)
Collecting safehttpx<0.2.0,>=0.1.6 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)
Collecting semantic-version~=2.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)
Collecting starlette<1.0,>=0.40.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)
Collecting tomlkit<0.14.0,>=0.12.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)
Collecting typer<1.0,>=0.12 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached typer-0.16.1-py3-none-any.whl.metadata (15 kB)
Collecting typing-extensions~=4.0 (from gradio==5.9.1->llava_mini==1.0.0)
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting fsspec (from gradio_client==1.5.2->llava_mini==1.0.0)
  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)
Collecting websockets<15.0,>=10.0 (from gradio_client==1.5.2->llava_mini==1.0.0)
  Using cached websockets-14.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting certifi (from httpx==0.28.1->llava_mini==1.0.0)
  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)
Collecting httpcore==1.* (from httpx==0.28.1->llava_mini==1.0.0)
  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)
Collecting idna (from httpx==0.28.1->llava_mini==1.0.0)
  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting scipy>=1.3.2 (from scikit-learn==1.2.2->llava_mini==1.0.0)
  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Collecting joblib>=1.1.1 (from scikit-learn==1.2.2->llava_mini==1.0.0)
  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)
Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.2->llava_mini==1.0.0)
  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)
Collecting filelock (from torch==2.1.2->llava_mini==1.0.0)
  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)
Collecting sympy (from torch==2.1.2->llava_mini==1.0.0)
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch==2.1.2->llava_mini==1.0.0)
  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)
Collecting triton==2.1.0 (from torch==2.1.2->llava_mini==1.0.0)
  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)
Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->llava_mini==1.0.0)
  Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting regex!=2019.12.17 (from transformers==4.43.1->llava_mini==1.0.0)
  Using cached regex-2025.8.29-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)
Collecting tqdm>=4.27 (from transformers==4.43.1->llava_mini==1.0.0)
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting exceptiongroup>=1.0.2 (from anyio<5.0,>=3.0->gradio==5.9.1->llava_mini==1.0.0)
  Using cached exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)
Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio==5.9.1->llava_mini==1.0.0)
  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
Collecting h11>=0.16 (from httpcore==1.*->httpx==0.28.1->llava_mini==1.0.0)
  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)
Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub->accelerate==0.29.0->llava_mini==1.0.0)
  Using cached hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)
Collecting python-dateutil>=2.8.2 (from pandas<3.0,>=1.0->gradio==5.9.1->llava_mini==1.0.0)
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio==5.9.1->llava_mini==1.0.0)
  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio==5.9.1->llava_mini==1.0.0)
  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting annotated-types>=0.6.0 (from pydantic->llava_mini==1.0.0)
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.33.2 (from pydantic->llava_mini==1.0.0)
  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting typing-inspection>=0.4.0 (from pydantic->llava_mini==1.0.0)
  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)
Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0)
  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)
Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0)
  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)
Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0)
  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio==5.9.1->llava_mini==1.0.0)
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0)
  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)
Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0)
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0)
  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
INFO: pip is looking at multiple versions of bitsandbytes to determine which version is compatible with other requirements. This could take a while.
Collecting bitsandbytes (from llava_mini==1.0.0)
  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)
  Using cached bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)
  Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)
Collecting wavedrom (from markdown2[all]->llava_mini==1.0.0)
  Using cached wavedrom-2.0.3.post3-py2.py3-none-any.whl
Collecting latex2mathml (from markdown2[all]->llava_mini==1.0.0)
  Using cached latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)
Collecting charset_normalizer<4,>=2 (from requests->llava_mini==1.0.0)
  Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)
Collecting urllib3<3,>=1.21.1 (from requests->llava_mini==1.0.0)
  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.1.2->llava_mini==1.0.0)
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting svgwrite (from wavedrom->markdown2[all]->llava_mini==1.0.0)
  Using cached svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)
Using cached accelerate-0.29.0-py3-none-any.whl (297 kB)
Using cached einops-0.6.1-py3-none-any.whl (42 kB)
Using cached einops_exts-0.0.4-py3-none-any.whl (3.9 kB)
Using cached gradio-5.9.1-py3-none-any.whl (57.2 MB)
Using cached gradio_client-1.5.2-py3-none-any.whl (320 kB)
Using cached httpx-0.28.1-py3-none-any.whl (73 kB)
Using cached scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)
Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Using cached timm-0.6.13-py3-none-any.whl (549 kB)
Using cached tokenizers-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)
Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)
Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)
Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)
Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)
Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)
Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)
Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)
Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)
Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)
Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)
Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)
Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)
Using cached transformers-4.43.1-py3-none-any.whl (9.4 MB)
Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)
Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)
Using cached anyio-4.10.0-py3-none-any.whl (107 kB)
Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)
Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)
Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)
Using cached hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)
Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)
Using cached orjson-3.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)
Using cached pandas-2.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)
Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)
Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)
Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)
Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)
Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)
Using cached starlette-0.47.3-py3-none-any.whl (72 kB)
Using cached tomlkit-0.13.3-py3-none-any.whl (38 kB)
Using cached typer-0.16.1-py3-none-any.whl (46 kB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached websockets-14.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)
Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)
Using cached click-8.2.1-py3-none-any.whl (102 kB)
Using cached exceptiongroup-1.3.0-py3-none-any.whl (16 kB)
Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)
Using cached h11-0.16.0-py3-none-any.whl (37 kB)
Using cached idna-3.10-py3-none-any.whl (70 kB)
Using cached joblib-1.5.2-py3-none-any.whl (308 kB)
Using cached packaging-25.0-py3-none-any.whl (66 kB)
Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)
Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)
Using cached regex-2025.8.29-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)
Using cached rich-14.1.0-py3-none-any.whl (243 kB)
Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)
Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)
Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Using cached ruff-0.12.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)
Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)
Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)
Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)
Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)
Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)
Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)
Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)
Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)
Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)
Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)
Using cached ffmpy-0.6.1-py3-none-any.whl (5.5 kB)
Using cached filelock-3.19.1-py3-none-any.whl (15 kB)
Using cached markdown2-2.5.4-py3-none-any.whl (49 kB)
Using cached latex2mathml-3.78.1-py3-none-any.whl (73 kB)
Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)
Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)
Using cached peft-0.17.1-py3-none-any.whl (504 kB)
Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)
Using cached requests-2.32.5-py3-none-any.whl (64 kB)
Using cached charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)
Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)
Using cached shortuuid-1.0.13-py3-none-any.whl (10 kB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached svgwrite-1.4.3-py3-none-any.whl (67 kB)
Building wheels for collected packages: llava_mini
  Building editable for llava_mini (pyproject.toml) ... done
  Created wheel for llava_mini: filename=llava_mini-1.0.0-0.editable-py3-none-any.whl size=10051 sha256=b9fce047ce4cec01ee365ceaa373530e8e1f7211cf38178fea6b7672b48706fb
  Stored in directory: /tmp/pip-ephem-wheel-cache-9au8062i/wheels/ae/66/f5/1923aeea1241989a088b3bf5d65dcb4d5902ac397b101c6793
Successfully built llava_mini
Installing collected packages: sentencepiece, pytz, pydub, mpmath, websockets, urllib3, tzdata, typing-extensions, tqdm, tomlkit, threadpoolctl, sympy, svgwrite, sniffio, six, shortuuid, shellingham, semantic-version, safetensors, ruff, regex, pyyaml, python-multipart, pygments, psutil, pillow, packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mdurl, markupsafe, markdown2, latex2mathml, joblib, idna, hf-xet, h11, fsspec, filelock, ffmpy, einops, click, charset_normalizer, certifi, annotated-types, aiofiles, wavedrom, uvicorn, typing-inspection, triton, scipy, requests, python-dateutil, pydantic-core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, jinja2, httpcore, exceptiongroup, einops-exts, scikit-learn, rich, pydantic, pandas, nvidia-cusolver-cu12, huggingface-hub, anyio, typer, torch, tokenizers, starlette, httpx, transformers, torchvision, safehttpx, gradio_client, fastapi, bitsandbytes, accelerate, timm, peft, gradio, llava_mini
Successfully installed accelerate-0.29.0 aiofiles-23.2.1 annotated-types-0.7.0 anyio-4.10.0 bitsandbytes-0.45.5 certifi-2025.8.3 charset_normalizer-3.4.3 click-8.2.1 einops-0.6.1 einops-exts-0.0.4 exceptiongroup-1.3.0 fastapi-0.116.1 ffmpy-0.6.1 filelock-3.19.1 fsspec-2025.7.0 gradio-5.9.1 gradio_client-1.5.2 h11-0.16.0 hf-xet-1.1.9 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.34.4 idna-3.10 jinja2-3.1.6 joblib-1.5.2 latex2mathml-3.78.1 llava_mini-1.0.0 markdown-it-py-4.0.0 markdown2-2.5.4 markupsafe-2.1.5 mdurl-0.1.2 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 orjson-3.11.3 packaging-25.0 pandas-2.3.2 peft-0.17.1 pillow-11.3.0 psutil-7.0.0 pydantic-2.11.7 pydantic-core-2.33.2 pydub-0.25.1 pygments-2.19.2 python-dateutil-2.9.0.post0 python-multipart-0.0.20 pytz-2025.2 pyyaml-6.0.2 regex-2025.8.29 requests-2.32.5 rich-14.1.0 ruff-0.12.11 safehttpx-0.1.6 safetensors-0.6.2 scikit-learn-1.2.2 scipy-1.15.3 semantic-version-2.10.0 sentencepiece-0.1.99 shellingham-1.5.4 shortuuid-1.0.13 six-1.17.0 sniffio-1.3.1 starlette-0.47.3 svgwrite-1.4.3 sympy-1.14.0 threadpoolctl-3.6.0 timm-0.6.13 tokenizers-0.19.0 tomlkit-0.13.3 torch-2.1.2 torchvision-0.16.2 tqdm-4.67.1 transformers-4.43.1 triton-2.1.0 typer-0.16.1 typing-extensions-4.15.0 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0 uvicorn-0.35.0 wavedrom-2.0.3.post3 websockets-14.2
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ pip install -e ".[train]"
Obtaining file:///home/ubuntu/LLaVA-Mini
  Installing build dependencies ... done
  Checking if build backend supports build_editable ... done
  Getting requirements to build editable ... done
  Preparing editable metadata (pyproject.toml) ... done
Requirement already satisfied: torch==2.1.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (2.1.2)
Requirement already satisfied: torchvision==0.16.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.16.2)
Requirement already satisfied: transformers==4.43.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (4.43.1)
Requirement already satisfied: tokenizers==0.19.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.19.0)
Requirement already satisfied: sentencepiece==0.1.99 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.1.99)
Requirement already satisfied: shortuuid in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (1.0.13)
Requirement already satisfied: accelerate==0.29.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.29.0)
Requirement already satisfied: peft in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.17.1)
Requirement already satisfied: bitsandbytes in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.45.5)
Requirement already satisfied: pydantic in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (2.11.7)
Requirement already satisfied: markdown2[all] in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (2.5.4)
Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (2.2.6)
Requirement already satisfied: scikit-learn==1.2.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (1.2.2)
Requirement already satisfied: gradio==5.9.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (5.9.1)
Requirement already satisfied: gradio_client==1.5.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (1.5.2)
Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (2.32.5)
Requirement already satisfied: httpx==0.28.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.28.1)
Requirement already satisfied: uvicorn in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.35.0)
Requirement already satisfied: fastapi in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.116.1)
Requirement already satisfied: einops==0.6.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.6.1)
Requirement already satisfied: einops-exts==0.0.4 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.0.4)
Requirement already satisfied: timm==0.6.13 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from llava_mini==1.0.0) (0.6.13)
Collecting deepspeed==0.12.6 (from llava_mini==1.0.0)
  Using cached deepspeed-0.12.6-py3-none-any.whl
Collecting ninja (from llava_mini==1.0.0)
  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)
Collecting wandb (from llava_mini==1.0.0)
  Using cached wandb-0.21.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from accelerate==0.29.0->llava_mini==1.0.0) (25.0)
Requirement already satisfied: psutil in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from accelerate==0.29.0->llava_mini==1.0.0) (7.0.0)
Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from accelerate==0.29.0->llava_mini==1.0.0) (6.0.2)
Requirement already satisfied: huggingface-hub in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from accelerate==0.29.0->llava_mini==1.0.0) (0.34.4)
Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from accelerate==0.29.0->llava_mini==1.0.0) (0.6.2)
Collecting hjson (from deepspeed==0.12.6->llava_mini==1.0.0)
  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)
Collecting py-cpuinfo (from deepspeed==0.12.6->llava_mini==1.0.0)
  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)
Collecting pynvml (from deepspeed==0.12.6->llava_mini==1.0.0)
  Using cached pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)
Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from deepspeed==0.12.6->llava_mini==1.0.0) (4.67.1)
Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (23.2.1)
Requirement already satisfied: anyio<5.0,>=3.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (4.10.0)
Requirement already satisfied: ffmpy in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (0.6.1)
Requirement already satisfied: jinja2<4.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (3.1.6)
Requirement already satisfied: markupsafe~=2.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (2.1.5)
Requirement already satisfied: orjson~=3.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (3.11.3)
Requirement already satisfied: pandas<3.0,>=1.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (2.3.2)
Requirement already satisfied: pillow<12.0,>=8.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (11.3.0)
Requirement already satisfied: pydub in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (0.25.1)
Requirement already satisfied: python-multipart>=0.0.18 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (0.0.20)
Requirement already satisfied: ruff>=0.2.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (0.12.11)
Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (0.1.6)
Requirement already satisfied: semantic-version~=2.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (2.10.0)
Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (0.47.3)
Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (0.13.3)
Requirement already satisfied: typer<1.0,>=0.12 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (0.16.1)
Requirement already satisfied: typing-extensions~=4.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio==5.9.1->llava_mini==1.0.0) (4.15.0)
Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio_client==1.5.2->llava_mini==1.0.0) (2025.7.0)
Requirement already satisfied: websockets<15.0,>=10.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from gradio_client==1.5.2->llava_mini==1.0.0) (14.2)
Requirement already satisfied: certifi in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from httpx==0.28.1->llava_mini==1.0.0) (2025.8.3)
Requirement already satisfied: httpcore==1.* in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from httpx==0.28.1->llava_mini==1.0.0) (1.0.9)
Requirement already satisfied: idna in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from httpx==0.28.1->llava_mini==1.0.0) (3.10)
Requirement already satisfied: scipy>=1.3.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava_mini==1.0.0) (1.15.3)
Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava_mini==1.0.0) (1.5.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava_mini==1.0.0) (3.6.0)
Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (3.19.1)
Requirement already satisfied: sympy in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (1.14.0)
Requirement already satisfied: networkx in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (3.4.2)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch==2.1.2->llava_mini==1.0.0) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->llava_mini==1.0.0) (12.9.86)
Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from transformers==4.43.1->llava_mini==1.0.0) (2025.8.29)
Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio==5.9.1->llava_mini==1.0.0) (1.3.0)
Requirement already satisfied: sniffio>=1.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio==5.9.1->llava_mini==1.0.0) (1.3.1)
Requirement already satisfied: h11>=0.16 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from httpcore==1.*->httpx==0.28.1->llava_mini==1.0.0) (0.16.0)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.29.0->llava_mini==1.0.0) (1.1.9)
Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==5.9.1->llava_mini==1.0.0) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==5.9.1->llava_mini==1.0.0) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==5.9.1->llava_mini==1.0.0) (2025.2)
Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from pydantic->llava_mini==1.0.0) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from pydantic->llava_mini==1.0.0) (2.33.2)
Requirement already satisfied: typing-inspection>=0.4.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from pydantic->llava_mini==1.0.0) (0.4.1)
Requirement already satisfied: click>=8.0.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0) (8.2.1)
Requirement already satisfied: shellingham>=1.3.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0) (1.5.4)
Requirement already satisfied: rich>=10.11.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0) (14.1.0)
Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio==5.9.1->llava_mini==1.0.0) (1.17.0)
Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0) (4.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0) (2.19.2)
Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==5.9.1->llava_mini==1.0.0) (0.1.2)
Requirement already satisfied: wavedrom in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from markdown2[all]->llava_mini==1.0.0) (2.0.3.post3)
Requirement already satisfied: latex2mathml in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from markdown2[all]->llava_mini==1.0.0) (3.78.1)
Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->deepspeed==0.12.6->llava_mini==1.0.0)
  Using cached nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)
Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from requests->llava_mini==1.0.0) (3.4.3)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from requests->llava_mini==1.0.0) (2.5.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from sympy->torch==2.1.2->llava_mini==1.0.0) (1.3.0)
Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->llava_mini==1.0.0)
  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)
Collecting platformdirs (from wandb->llava_mini==1.0.0)
  Using cached platformdirs-4.4.0-py3-none-any.whl.metadata (12 kB)
Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb->llava_mini==1.0.0)
  Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
Collecting sentry-sdk>=2.0.0 (from wandb->llava_mini==1.0.0)
  Using cached sentry_sdk-2.35.1-py2.py3-none-any.whl.metadata (10 kB)
Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->llava_mini==1.0.0)
  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)
Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->llava_mini==1.0.0)
  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)
Requirement already satisfied: svgwrite in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from wavedrom->markdown2[all]->llava_mini==1.0.0) (1.4.3)
Using cached hjson-3.1.0-py3-none-any.whl (54 kB)
Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)
Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)
Using cached pynvml-12.0.0-py3-none-any.whl (26 kB)
Using cached nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)
Using cached wandb-0.21.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)
Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)
Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)
Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)
Using cached smmap-5.0.2-py3-none-any.whl (24 kB)
Using cached sentry_sdk-2.35.1-py2.py3-none-any.whl (363 kB)
Using cached platformdirs-4.4.0-py3-none-any.whl (18 kB)
Building wheels for collected packages: llava_mini
  Building editable for llava_mini (pyproject.toml) ... done
  Created wheel for llava_mini: filename=llava_mini-1.0.0-0.editable-py3-none-any.whl size=10051 sha256=33784ab04111e070473b76c12618f5978d48f1c88b9bce265c9dadd33daa1ce5
  Stored in directory: /tmp/pip-ephem-wheel-cache-ix4qzn9b/wheels/ae/66/f5/1923aeea1241989a088b3bf5d65dcb4d5902ac397b101c6793
Successfully built llava_mini
Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, smmap, sentry-sdk, pynvml, protobuf, platformdirs, ninja, gitdb, gitpython, wandb, deepspeed, llava_mini
  Attempting uninstall: llava_mini
    Found existing installation: llava_mini 1.0.0
    Uninstalling llava_mini-1.0.0:
      Successfully uninstalled llava_mini-1.0.0
Successfully installed deepspeed-0.12.6 gitdb-4.0.12 gitpython-3.1.45 hjson-3.1.0 llava_mini-1.0.0 ninja-1.13.0 nvidia-ml-py-12.575.51 platformdirs-4.4.0 protobuf-6.32.0 py-cpuinfo-9.0.0 pynvml-12.0.0 sentry-sdk-2.35.1 smmap-5.0.2 wandb-0.21.2
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ pip install flash-attn --no-build-isolation
Collecting flash-attn
  Using cached flash_attn-2.8.3.tar.gz (8.4 MB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: torch in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from flash-attn) (2.1.2)
Requirement already satisfied: einops in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from flash-attn) (0.6.1)
Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (3.19.1)
Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (4.15.0)
Requirement already satisfied: sympy in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (1.14.0)
Requirement already satisfied: networkx in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (3.4.2)
Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (3.1.6)
Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (2025.7.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.9.86)
Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)
Building wheels for collected packages: flash-attn
  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for flash-attn (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> [355 lines of output]

      A module that was compiled using NumPy 1.x cannot be run in
      NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
      versions of NumPy, modules must be compiled with NumPy 2.0.
      Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

      If you are a user of the module, the easiest solution will be to
      downgrade to 'numpy<2' or try to upgrade the affected module.
      We expect that some modules will need time to support NumPy 2.

      Traceback (most recent call last):  File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 35, in <module>
        File "/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/setup.py", line 22, in <module>
          import torch
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
          from .functional import *  # noqa: F403
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
          import torch.nn.functional as F
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
          from .modules import *  # noqa: F403
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
          from .transformer import TransformerEncoder, TransformerDecoder, \
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
          device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
        device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),


      torch.__version__  = 2.1.2+cu121


      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.
      !!

              ********************************************************************************
              Requirements should be satisfied by a PEP 517 installer.
              If you are using pip, you can try `pip install --use-pep517`.
              ********************************************************************************

      !!
        dist.fetch_build_eggs(dist.setup_requires)
      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.
      !!

              ********************************************************************************
              Please consider removing the following classifiers in favor of a SPDX license expression:

              License :: OSI Approved :: BSD License

              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.
              ********************************************************************************

      !!
        self._finalize_license_expression()
      running bdist_wheel
      Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
      Precompiled wheel not found. Building from source...
      running build
      running build_py
      creating build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/benchmark_split_kv.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/generate_kernels.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/test_flash_attn.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/test_kvcache.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/padding.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/test_util.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/benchmark_attn.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/setup.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/benchmark_mla_decode.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/test_attn_kvcache.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/__init__.py -> build/lib.linux-x86_64-cpython-310/hopper
      copying hopper/benchmark_flash_attention_fp8.py -> build/lib.linux-x86_64-cpython-310/hopper
      creating build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      copying flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn
      creating build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/fwd_decode.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/bwd_prefill_onekernel.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/interface_fa.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/test.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/fp8.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/bwd_prefill_fused.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/bwd_ref.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/utils.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/fwd_prefill.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/bwd_prefill.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/fwd_ref.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/bwd_prefill_split.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/train.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      copying flash_attn/flash_attn_triton_amd/bench.py -> build/lib.linux-x86_64-cpython-310/flash_attn/flash_attn_triton_amd
      creating build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/btlm.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/bigcode.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/models
      creating build/lib.linux-x86_64-cpython-310/flash_attn/layers
      copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-310/flash_attn/layers
      copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-310/flash_attn/layers
      copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/layers
      creating build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/modules
      creating build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/flash_bwd_postprocess.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/mask.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/interface.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/hopper_helpers.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/named_barrier.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/flash_bwd.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/tile_scheduler.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/block_info.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/fast_math.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/blackwell_helpers.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/mma_sm100_desc.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/utils.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/flash_fwd.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/flash_bwd_preprocess.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/softmax.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/ampere_helpers.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/pack_gqa.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/flash_fwd_sm100.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/pipeline.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      copying flash_attn/cute/seqlen_info.py -> build/lib.linux-x86_64-cpython-310/flash_attn/cute
      creating build/lib.linux-x86_64-cpython-310/flash_attn/losses
      copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/flash_attn/losses
      copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/losses
      creating build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/testing.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/library.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/torch.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/utils
      creating build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops
      creating build/lib.linux-x86_64-cpython-310/flash_attn/ops/triton
      copying flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops/triton
      copying flash_attn/ops/triton/layer_norm.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops/triton
      copying flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops/triton
      copying flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops/triton
      copying flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops/triton
      copying flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops/triton
      copying flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-cpython-310/flash_attn/ops/triton
      running build_ext
      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.8) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.
        warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))
      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.8
        warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')
      building 'flash_attn_2_cuda' extension
      creating /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn
      creating /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src
      Emitting ninja build file /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/build.ninja...
      Compiling objects...
      Using envvar MAX_JOBS (15) as the number of workers...
      [1/73] c++ -MMD -MF /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/flash_api.o.d -pthread -B /home/ubuntu/miniconda3/envs/llavamini/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/ubuntu/miniconda3/envs/llavamini/include -fPIC -O2 -isystem /home/ubuntu/miniconda3/envs/llavamini/include -fPIC -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/flash_api.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      FAILED: [code=1] /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/flash_api.o
      c++ -MMD -MF /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/flash_api.o.d -pthread -B /home/ubuntu/miniconda3/envs/llavamini/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/ubuntu/miniconda3/envs/llavamini/include -fPIC -O2 -isystem /home/ubuntu/miniconda3/envs/llavamini/include -fPIC -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/flash_api.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp: In function ‘std::vector<at::Tensor> flash::mha_fwd(at::Tensor&, const at::Tensor&, const at::Tensor&, std::optional<at::Tensor>&, std::optional<at::Tensor>&, float, float, bool, int, int, float, bool, std::optional<at::Generator>)’:
      /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp:489:13: error: invalid initialization of reference of type ‘const c10::optional<at::Generator>&’ from expression of type ‘std::optional<at::Generator>’
        489 |             gen_, at::cuda::detail::getDefaultCUDAGenerator());
            |             ^~~~
      In file included from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/DeprecatedTypeProperties.h:9,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:33,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/utils/variadic.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/detail/static.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/python.h:3,
                       from /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp:6:
      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/Generator.h:167:75: note: in passing argument 1 of ‘T* at::get_generator_or_default(const c10::optional<at::Generator>&, const at::Generator&) [with T = at::CUDAGeneratorImpl]’
        167 | static inline T* get_generator_or_default(const c10::optional<Generator>& gen, const Generator& default_gen) {
            |                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~
      /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp: In function ‘std::vector<at::Tensor> flash::mha_varlen_fwd(at::Tensor&, const at::Tensor&, const at::Tensor&, std::optional<at::Tensor>&, const at::Tensor&, const at::Tensor&, std::optional<at::Tensor>&, std::optional<const at::Tensor>&, std::optional<at::Tensor>&, std::optional<at::Tensor>&, int, int, float, float, bool, bool, int, int, float, bool, std::optional<at::Generator>)’:
      /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp:729:13: error: invalid initialization of reference of type ‘const c10::optional<at::Generator>&’ from expression of type ‘std::optional<at::Generator>’
        729 |             gen_, at::cuda::detail::getDefaultCUDAGenerator());
            |             ^~~~
      In file included from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/DeprecatedTypeProperties.h:9,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:33,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/utils/variadic.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/detail/static.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/python.h:3,
                       from /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp:6:
      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/Generator.h:167:75: note: in passing argument 1 of ‘T* at::get_generator_or_default(const c10::optional<at::Generator>&, const at::Generator&) [with T = at::CUDAGeneratorImpl]’
        167 | static inline T* get_generator_or_default(const c10::optional<Generator>& gen, const Generator& default_gen) {
            |                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~
      /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp: In function ‘std::vector<at::Tensor> flash::mha_bwd(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, std::optional<at::Tensor>&, std::optional<at::Tensor>&, std::optional<at::Tensor>&, std::optional<at::Tensor>&, float, float, bool, int, int, float, bool, std::optional<at::Generator>, std::optional<at::Tensor>&)’:
      /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp:937:9: error: invalid initialization of reference of type ‘const c10::optional<at::Generator>&’ from expression of type ‘std::optional<at::Generator>’
        937 |         gen_, at::cuda::detail::getDefaultCUDAGenerator());
            |         ^~~~
      In file included from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/DeprecatedTypeProperties.h:9,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:33,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/utils/variadic.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/detail/static.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/python.h:3,
                       from /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp:6:
      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/Generator.h:167:75: note: in passing argument 1 of ‘T* at::get_generator_or_default(const c10::optional<at::Generator>&, const at::Generator&) [with T = at::CUDAGeneratorImpl]’
        167 | static inline T* get_generator_or_default(const c10::optional<Generator>& gen, const Generator& default_gen) {
            |                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~
      /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp: In function ‘std::vector<at::Tensor> flash::mha_varlen_bwd(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, std::optional<at::Tensor>&, std::optional<at::Tensor>&, std::optional<at::Tensor>&, const at::Tensor&, const at::Tensor&, std::optional<at::Tensor>&, int, int, float, float, bool, bool, int, int, float, bool, std::optional<at::Generator>, std::optional<at::Tensor>&)’:
      /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp:1166:9: error: invalid initialization of reference of type ‘const c10::optional<at::Generator>&’ from expression of type ‘std::optional<at::Generator>’
       1166 |         gen_, at::cuda::detail::getDefaultCUDAGenerator());
            |         ^~~~
      In file included from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/DeprecatedTypeProperties.h:9,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:33,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/utils/variadic.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/detail/static.h:3,
                       from /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/python.h:3,
                       from /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/flash_api.cpp:6:
      /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/ATen/core/Generator.h:167:75: note: in passing argument 1 of ‘T* at::get_generator_or_default(const c10::optional<at::Generator>&, const at::Generator&) [with T = at::CUDAGeneratorImpl]’
        167 | static inline T* get_generator_or_default(const c10::optional<Generator>& gen, const Generator& default_gen) {
            |                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~
      [2/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim32_bf16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim32_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      [3/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim192_fp16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim192_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      [4/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim192_bf16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim192_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      [5/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim256_bf16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim256_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      FAILED: [code=255] /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim256_bf16_causal_sm80.o
      /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim256_bf16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim256_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      Killed
      [6/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim128_fp16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim128_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      FAILED: [code=255] /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim128_fp16_causal_sm80.o
      /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim128_fp16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim128_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      Killed
      [7/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      [8/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim256_fp16_causal_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim256_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      [9/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      FAILED: [code=255] /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o
      /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      Killed
      [10/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      [11/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      FAILED: [code=255] /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o
      /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      Killed
      Killed
      [12/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      [13/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      FAILED: [code=255] /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o
      /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      Killed
      [14/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      [15/73] /usr/bin/nvcc  -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src -I/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/cutlass/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/TH -I/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/include/THC -I/home/ubuntu/miniconda3/envs/llavamini/include/python3.10 -c -c /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.cu -o /tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/build/temp.linux-x86_64-cpython-310/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_100,code=sm_100 -gencode arch=compute_120,code=sm_120 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0
      ninja: build stopped: subcommand failed.
      Traceback (most recent call last):
        File "/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/setup.py", line 486, in run
          urllib.request.urlretrieve(wheel_url, wheel_filename)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/urllib/request.py", line 241, in urlretrieve
          with contextlib.closing(urlopen(url, data)) as fp:
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/urllib/request.py", line 216, in urlopen
          return opener.open(url, data, timeout)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/urllib/request.py", line 525, in open
          response = meth(req, response)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/urllib/request.py", line 634, in http_response
          response = self.parent.error(
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/urllib/request.py", line 563, in error
          return self._call_chain(*args)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/urllib/request.py", line 496, in _call_chain
          result = func(*args)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/urllib/request.py", line 643, in http_error_default
          raise HTTPError(req.full_url, code, msg, hdrs, fp)
      urllib.error.HTTPError: HTTP Error 404: Not Found

      During handling of the above exception, another exception occurred:

      Traceback (most recent call last):
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 2100, in _run_ninja_build
          subprocess.run(
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/subprocess.py", line 526, in run
          raise CalledProcessError(retcode, process.args,
      subprocess.CalledProcessError: Command '['ninja', '-v', '-j', '15']' returned non-zero exit status 255.

      The above exception was the direct cause of the following exception:

      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 35, in <module>
        File "/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/setup.py", line 526, in <module>
          setup(
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/__init__.py", line 117, in setup
          return distutils.core.setup(**attrs)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/core.py", line 186, in setup
          return run_commands(dist)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/core.py", line 202, in run_commands
          dist.run_commands()
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/dist.py", line 1002, in run_commands
          self.run_command(cmd)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/dist.py", line 1104, in run_command
          super().run_command(command)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/dist.py", line 1021, in run_command
          cmd_obj.run()
        File "/tmp/pip-install-7vkm9x7z/flash-attn_b949cb57d4da4b4d8c919cc6a29c00cb/setup.py", line 503, in run
          super().run()
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/command/bdist_wheel.py", line 370, in run
          self.run_command("build")
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/cmd.py", line 357, in run_command
          self.distribution.run_command(command)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/dist.py", line 1104, in run_command
          super().run_command(command)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/dist.py", line 1021, in run_command
          cmd_obj.run()
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/command/build.py", line 135, in run
          self.run_command(cmd_name)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/cmd.py", line 357, in run_command
          self.distribution.run_command(command)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/dist.py", line 1104, in run_command
          super().run_command(command)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/dist.py", line 1021, in run_command
          cmd_obj.run()
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/command/build_ext.py", line 99, in run
          _build_ext.run(self)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py", line 368, in run
          self.build_extensions()
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 873, in build_extensions
          build_ext.build_extensions(self)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py", line 484, in build_extensions
          self._build_extensions_serial()
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py", line 510, in _build_extensions_serial
          self.build_extension(ext)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/command/build_ext.py", line 264, in build_extension
          _build_ext.build_extension(self, ext)
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py", line 565, in build_extension
          objects = self.compiler.compile(
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 686, in unix_wrap_ninja_compile
          _write_ninja_file_and_compile_objects(
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1774, in _write_ninja_file_and_compile_objects
          _run_ninja_build(
        File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 2116, in _run_ninja_build
          raise RuntimeError(message) from e
      RuntimeError: Error compiling objects for extension
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for flash-attn
  Running setup.py clean for flash-attn
Failed to build flash-attn
ERROR: Failed to build installable wheels for some pyproject.toml based projects (flash-attn)
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ pip install flash-attn==2.3.4 --no-build-isolation
Collecting flash-attn==2.3.4
  Using cached flash_attn-2.3.4-cp310-cp310-linux_x86_64.whl
Requirement already satisfied: torch in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from flash-attn==2.3.4) (2.1.2)
Requirement already satisfied: einops in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from flash-attn==2.3.4) (0.6.1)
Requirement already satisfied: packaging in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from flash-attn==2.3.4) (25.0)
Requirement already satisfied: ninja in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from flash-attn==2.3.4) (1.13.0)
Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (3.19.1)
Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (4.15.0)
Requirement already satisfied: sympy in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (1.14.0)
Requirement already satisfied: networkx in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (3.4.2)
Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (3.1.6)
Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (2025.7.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from torch->flash-attn==2.3.4) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn==2.3.4) (12.9.86)
Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from jinja2->torch->flash-attn==2.3.4) (2.1.5)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.3.4) (1.3.0)
Installing collected packages: flash-attn
Successfully installed flash-attn-2.3.4
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ # Image Understanding
CUDA_VISIBLE_DEVICES=0 python llavamini/eval/run_llava_mini.py \
    --model-path  ICTNLP/llava-mini-llama-3.1-8b \
    --image-file llavamini/serve/examples/baby_cake.png \
    --conv-mode llava_llama_3_1 --model-name "llava-mini" \
    --query "What's the text on the cake?"

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/ubuntu/LLaVA-Mini/llavamini/eval/run_llava_mini.py", line 2, in <module>
    import torch
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
Traceback (most recent call last):
  File "/home/ubuntu/LLaVA-Mini/llavamini/eval/run_llava_mini.py", line 22, in <module>
    from decord import VideoReader, cpu
ModuleNotFoundError: No module named 'decord'
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ !pip install decord
pip install flash-attn==2.3.4 --no-build-isolation install decord
Requirement already satisfied: flash-attn==2.3.4 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (2.3.4)
ERROR: Could not find a version that satisfies the requirement install (from versions: none)
ERROR: No matching distribution found for install
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ pip install decord
Collecting decord
  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)
Requirement already satisfied: numpy>=1.14.0 in /home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages (from decord) (2.2.6)
Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)
Installing collected packages: decord
Successfully installed decord-0.6.0
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ # Image Understanding
CUDA_VISIBLE_DEVICES=0 python llavamini/eval/run_llava_mini.py \
    --model-path  ICTNLP/llava-mini-llama-3.1-8b \
    --image-file llavamini/serve/examples/baby_cake.png \
    --conv-mode llava_llama_3_1 --model-name "llava-mini" \
    --query "What's the text on the cake?"

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/ubuntu/LLaVA-Mini/llavamini/eval/run_llava_mini.py", line 2, in <module>
    import torch
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
Traceback (most recent call last):
  File "/home/ubuntu/LLaVA-Mini/llavamini/eval/run_llava_mini.py", line 266, in <module>
    eval_model(args)
  File "/home/ubuntu/LLaVA-Mini/llavamini/eval/run_llava_mini.py", line 155, in eval_model
    tokenizer, model, image_processor, context_len = load_pretrained_model(
  File "/home/ubuntu/LLaVA-Mini/llavamini/model/builder.py", line 111, in load_pretrained_model
    model = LlavaMiniLlamaForCausalLM.from_pretrained(
  File "/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3775, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/ubuntu/LLaVA-Mini/llavamini/model/language_model/llavamini_llama.py", line 46, in __init__
    self.model = LlavaMiniLlamaModel(config)
  File "/home/ubuntu/LLaVA-Mini/llavamini/model/language_model/llavamini_llama.py", line 38, in __init__
    super(LlavaMiniLlamaModel, self).__init__(config)
  File "/home/ubuntu/LLaVA-Mini/llavamini/model/llavamini_arch.py", line 187, in __init__
    self.build_compressor(config)
  File "/home/ubuntu/LLaVA-Mini/llavamini/model/llavamini_arch.py", line 198, in build_compressor
    self.compressor=Resampler(
  File "/home/ubuntu/LLaVA-Mini/llavamini/model/llavamini_arch.py", line 125, in __init__
    self.pos_embed = nn.Parameter(torch.from_numpy(get_2d_sincos_pos_embed(embed_dim, grid_size))).requires_grad_(False)
RuntimeError: Numpy is not available
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ python -m pip uninstall -y numpy
python -m pip install --no-cache-dir "numpy==1.26.4"
Found existing installation: numpy 2.2.6
Uninstalling numpy-2.2.6:
  Successfully uninstalled numpy-2.2.6
Collecting numpy==1.26.4
  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 233.6 MB/s eta 0:00:00
Installing collected packages: numpy
Successfully installed numpy-1.26.4
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ # Image Understanding
CUDA_VISIBLE_DEVICES=0 python llavamini/eval/run_llava_mini.py \
    --model-path  ICTNLP/llava-mini-llama-3.1-8b \
    --image-file llavamini/serve/examples/baby_cake.png \
    --conv-mode llava_llama_3_1 --model-name "llava-mini" \
    --query "What's the text on the cake?"
#Vision Tokens: 1
Loading checkpoint shards: 100%|███████████████████████████████████████| 4/4 [00:21<00:00,  5.26s/it]
Some weights of the model checkpoint at ICTNLP/llava-mini-llama-3.1-8b were not used when initializing LlavaMiniLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']
- This IS expected if you are initializing LlavaMiniLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaMiniLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_3_1, using llava_llama_3_1
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be removed and `position_embeddings` will be mandatory.
/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Congratulations Kate & Luke on your upcoming arrival
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$ # Video Understanding
CUDA_VISIBLE_DEVICES=0 python llavamini/eval/run_llava_mini.py \
    --model-path  ICTNLP/llava-mini-llama-3.1-8b \
    --video-file llavamini/serve/examples/fifa.mp4 \
    --conv-mode llava_llama_3_1 --model-name "llava-mini" \
    --query "What happened in this video?"
#Vision Tokens: 1
Loading checkpoint shards: 100%|███████████████████████████████████████| 4/4 [00:03<00:00,  1.15it/s]
Some weights of the model checkpoint at ICTNLP/llava-mini-llama-3.1-8b were not used when initializing LlavaMiniLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']
- This IS expected if you are initializing LlavaMiniLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaMiniLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is llava_llama_3_1, using llava_llama_3_1
torch.Size([1, 22, 3, 336, 336])
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be removed and `position_embeddings` will be mandatory.
/home/ubuntu/miniconda3/envs/llavamini/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The video captures a moment of triumph for the Argentine national football team, celebrating their victory at the FIFA World Cup in Qatar 2022. The team is seen on a stage, holding up their trophy, which is the iconic FIFA World Cup trophy, signifying their achievement in the tournament. The celebration is marked by the players' joyful expressions and the bright lights of the stadium in the background, which suggest a large-scale event with a significant audience. The presence of the trophy and the team's attire, along with the stadium lights, indicates that this is a post-match celebration following a successful game, likely the final, given the context of the World Cup. The team's attire, featuring the colors of the Argentine flag, signifies national pride and unity. The event is a significant moment in sports, representing the culmination of a team's hard work and dedication to reach the pinnacle of international football.
(llavamini) ubuntu@129-213-151-56:~/LLaVA-Mini$